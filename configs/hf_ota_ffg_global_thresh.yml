merge_method: ota
base_model: meta-llama/Meta-Llama-3.1-8B

parameters:
  epsilon: 1e-26 # Default epsilon for OTA, can be tuned
  normalise: none
  power: 0.5
  fallback_to_base: true
  rescale: false
  # approximate_norm: true
  # rescale_relative_threshold: 1e-6
  precond_threshold: 6.0e-19
  # masked_task_vector_merge_method: linear

models:
  - model: pmahdavi/Llama-3.1-8B-math-reasoning
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-math-reasoning/export/exp_avg_sq.safetensors
      rank1_approx: true
  - model: pmahdavi/Llama-3.1-8B-coding-tulu3-ebs128-lr5e6-wsdcr0p4
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-coding-tulu3-ebs128-lr5e6-wsdcr0p4/export_full_state_checkpoint-1100/exp_avg_sq.safetensors
      rank1_approx: true
  - model: pmahdavi/Llama-3.1-8B-precise-if
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-precise-if/export/exp_avg_sq.safetensors
      rank1_approx: true
  - model: pmahdavi/Llama-3.1-8B-general
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-general/export/exp_avg_sq.safetensors
      rank1_approx: true
  - model: pmahdavi/Llama-3.1-8B-knowledge-recall
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-knowledge-recall/export/exp_avg_sq.safetensors
      rank1_approx: true

dtype: bfloat16
tokenizer_source: union 
merge_method: ota
base_model: meta-llama/Meta-Llama-3.1-8B

parameters:
  epsilon: 1e-26 # Default epsilon for OTA, can be tuned
  normalise: none
  power: 0.5
  fallback_to_base: true
  rescale: false
  precond_threshold: 5e-22

models:
  - model: pmahdavi/Llama-3.1-8B-math-reasoning
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-math-reasoning/export/exp_avg_sq.safetensors
  - model: pmahdavi/Llama-3.1-8B-coding
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-coding/export/exp_avg_sq.safetensors
  - model: pmahdavi/Llama-3.1-8B-precise-if
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-precise-if/export/exp_avg_sq.safetensors
  - model: pmahdavi/Llama-3.1-8B-general
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-general/export/exp_avg_sq.safetensors
  - model: pmahdavi/Llama-3.1-8B-knowledge-recall
    parameters:
      preconditioner_path: pmahdavi/Llama-3.1-8B-knowledge-recall/export/exp_avg_sq.safetensors

dtype: bfloat16
tokenizer_source: union 
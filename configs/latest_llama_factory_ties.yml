merge_method: ties
base_model: meta-llama/Meta-Llama-3.1-8B
parameters:
  density: 0.65 # Default density for TIES, can be tuned
  weight: 1.0    # Equal weighting for all models before TIES processing

models:
  - model: pmahdavi/Llama-3.1-8B-math-reasoning
    parameters:
      weight: 1.0
  - model: pmahdavi/Llama-3.1-8B-coding
    parameters:
      weight: 1.0
  - model: pmahdavi/Llama-3.1-8B-precise-if
    parameters:
      weight: 1.0
  - model: pmahdavi/Llama-3.1-8B-general
    parameters:
      weight: 1.0
  - model: pmahdavi/Llama-3.1-8B-knowledge-recall
    parameters:
      weight: 1.0

dtype: bfloat16
tokenizer_source: union 
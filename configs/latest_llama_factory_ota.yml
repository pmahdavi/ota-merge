merge_method: ota

parameters:
  epsilon: 1e-8 # Default epsilon for OTA, can be tuned
  normalise: global
  power: 0.5

models:
  - model: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_coding_full_ebs128_lr5e-06/checkpoint-1111
    parameters:
      weight: 1.0
      preconditioner_path: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_coding_full_ebs128_lr5e-06/export_full_state_checkpoint-1111/exp_avg_sq.safetensors
  - model: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_general_full_ebs128_lr5e-06/checkpoint-912
    parameters:
      weight: 1.0
      preconditioner_path: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_general_full_ebs128_lr5e-06/export_full_state_checkpoint-912/exp_avg_sq.safetensors
  - model: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_knowledge_recall_full_ebs128_lr5e-06/checkpoint-820
    parameters:
      weight: 1.0
      preconditioner_path: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_knowledge_recall_full_ebs128_lr5e-06/export_full_state_checkpoint-820/exp_avg_sq.safetensors
  - model: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_math_reasoning_full_ebs128_lr5e-06/checkpoint-2611
    parameters:
      weight: 1.0
      preconditioner_path: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_math_reasoning_full_ebs128_lr5e-06/export_full_state_checkpoint-2611/exp_avg_sq.safetensors
  - model: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_precise_if_full_ebs128_lr1e-05/checkpoint-234
    parameters:
      weight: 1.0
      preconditioner_path: /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_precise_if_full_ebs128_lr1e-05/export_full_state_checkpoint-234/exp_avg_sq.safetensors

dtype: bfloat16
tokenizer_source: union 